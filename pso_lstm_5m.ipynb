{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9e2ea047",
      "metadata": {},
      "source": [
        "# PSO-LSTMによる株価指数の騰落率予測（5分足）\n",
        "\n",
        "このノートブックは、論文「Enhancing stock index prediction: A hybrid LSTM-PSO model for improved forecasting accuracy」に基づき、\n",
        "PSOでLSTMのハイパーパラメータを最適化し、次時点の騰落率（pct_change）を予測します。\n",
        "\n",
        "- データ: yfinance（5分足）\n",
        "- ルックバック: 5分 / 30分 / 60分\n",
        "- マクロ指標: USD/JPY、金利（^IRX）\n",
        "- 評価指標: RMSE / MAE / MAPE / R2\n",
        "\n",
        "> 注意: 5分足データは取得可能期間が短いです。必要に応じて期間を調整してください。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a0245dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import pywt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "import ta\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import pyswarms as ps\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# GPUがある場合はメモリ成長を有効化\n",
        "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if physical_gpus:\n",
        "    for gpu in physical_gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    print(f\"GPU利用: {len(physical_gpus)}台\")\n",
        "else:\n",
        "    print(\"GPUなし: CPUで実行します\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a74e3203",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== 設定 =====\n",
        "INDEX_TICKERS = [\"^GSPC\"]  # 例: S&P500\n",
        "FX_TICKER = \"USDJPY=X\"\n",
        "RATE_TICKER = \"^IRX\"  # 米国短期金利（代替指標）\n",
        "\n",
        "BASE_INTERVAL = \"5m\"\n",
        "BASE_PERIOD = \"60d\"  # 5分足の取得可能期間に合わせる\n",
        "\n",
        "RESAMPLE_MINUTES = [5, 30, 60]\n",
        "LOOKBACK_STEPS = {\n",
        "    5: 5,    # 5分足 -> 5ステップ\n",
        "    30: 30,  # 30分足 -> 30ステップ\n",
        "    60: 60,  # 60分足 -> 60ステップ\n",
        "}\n",
        "\n",
        "TRAIN_RATIO = 0.8\n",
        "VAL_RATIO = 0.2\n",
        "BATCH_SIZE = 32\n",
        "REPEATS = 1  # 1回 or 3回\n",
        "\n",
        "# PSO設定（論文値はK=50）\n",
        "PSO_PARTICLES = 20\n",
        "PSO_ITERS = 10\n",
        "PSO_W = 0.8\n",
        "PSO_C1 = 1.5\n",
        "PSO_C2 = 1.5\n",
        "\n",
        "# PSO探索範囲\n",
        "NEURON_BOUNDS = (50, 300)\n",
        "EPOCH_BOUNDS = (50, 300)\n",
        "LAYER_BOUNDS = (1, 3)\n",
        "\n",
        "print(\"設定完了\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8af430e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _normalize_index(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_convert(None)\n",
        "    return df\n",
        "\n",
        "\n",
        "def download_price_data(ticker: str, interval: str, period: str) -> pd.DataFrame:\n",
        "    df = yf.download(ticker, interval=interval, period=period, auto_adjust=False, progress=False)\n",
        "    # 概要\n",
        "    if df.empty:\n",
        "        raise ValueError(f\"データ取得に失敗: {ticker}\")\n",
        "    df = _normalize_index(df)\n",
        "    df = df.rename(columns=str.lower)\n",
        "    return df\n",
        "\n",
        "\n",
        "def download_macro_daily(ticker: str, period_years: int = 5) -> pd.Series:\n",
        "    df = yf.download(ticker, interval=\"1d\", period=f\"{period_years}y\", auto_adjust=False, progress=False)\n",
        "    if df.empty:\n",
        "        raise ValueError(f\"マクロデータ取得に失敗: {ticker}\")\n",
        "    df = _normalize_index(df)\n",
        "    series = df[\"Close\"].copy()\n",
        "    series.name = ticker\n",
        "    return series\n",
        "\n",
        "\n",
        "def resample_ohlcv(df: pd.DataFrame, minutes: int) -> pd.DataFrame:\n",
        "    if minutes == 5:\n",
        "        return df.copy()\n",
        "    rule = f\"{minutes}min\"\n",
        "    ohlc = df[\"open\"].resample(rule).first()\n",
        "    high = df[\"high\"].resample(rule).max()\n",
        "    low = df[\"low\"].resample(rule).min()\n",
        "    close = df[\"close\"].resample(rule).last()\n",
        "    volume = df[\"volume\"].resample(rule).sum()\n",
        "    out = pd.concat([ohlc, high, low, close, volume], axis=1)\n",
        "    out.columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
        "    return out.dropna()\n",
        "\n",
        "\n",
        "def merge_macro_features(price_df: pd.DataFrame, fx: pd.Series, rate: pd.Series) -> pd.DataFrame:\n",
        "    df = price_df.copy()\n",
        "    fx = fx.reindex(df.index, method=\"ffill\")\n",
        "    rate = rate.reindex(df.index, method=\"ffill\")\n",
        "    df[\"usd_jpy\"] = fx\n",
        "    df[\"interest_rate\"] = rate\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "466315eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def wavelet_denoise(series: pd.Series, wavelet: str = \"haar\", level: int = 3) -> pd.Series:\n",
        "    if isinstance(series, pd.DataFrame):\n",
        "        series = series.squeeze()\n",
        "        if isinstance(series, pd.DataFrame):\n",
        "            series = series.iloc[:, 0]\n",
        "    arr = np.asarray(series, dtype=np.float64).ravel().copy()\n",
        "    coeffs = pywt.wavedec(arr, wavelet, level=level)\n",
        "    detail_coeffs = coeffs[1:]\n",
        "    sigma = np.median(np.abs(detail_coeffs[-1])) / 0.6745 if len(detail_coeffs) > 0 else 0\n",
        "    uthresh = sigma * np.sqrt(2 * np.log(len(arr))) if sigma > 0 else 0\n",
        "    coeffs[1:] = [pywt.threshold(c, value=uthresh, mode=\"soft\") for c in coeffs[1:]]\n",
        "    reconstructed = pywt.waverec(coeffs, wavelet)\n",
        "    reconstructed = reconstructed[: len(arr)]\n",
        "    name = getattr(series, \"name\", None)\n",
        "    index = series.index if hasattr(series, \"index\") else pd.RangeIndex(len(reconstructed))\n",
        "    return pd.Series(reconstructed, index=index, name=name)\n",
        "\n",
        "\n",
        "def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    idx = out.index\n",
        "    for col in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
        "        if col in out.columns:\n",
        "            out[col] = pd.Series(np.asarray(out[col], dtype=float).ravel(), index=idx)\n",
        "\n",
        "    macd = ta.trend.MACD(close=out[\"close\"].squeeze())\n",
        "    out[\"macd\"] = macd.macd_diff().values.reshape(-1, 1)\n",
        "\n",
        "    out[\"cci\"] = ta.trend.cci(out[\"high\"].squeeze(), out[\"low\"].squeeze(), out[\"close\"].squeeze()).values.reshape(-1, 1)\n",
        "    out[\"atr\"] = ta.volatility.average_true_range(out[\"high\"].squeeze(), out[\"low\"].squeeze(), out[\"close\"].squeeze()).values.reshape(-1, 1)\n",
        "\n",
        "    boll = ta.volatility.BollingerBands(close=out[\"close\"].squeeze())\n",
        "    out[\"boll\"] = boll.bollinger_mavg().values.reshape(-1, 1)\n",
        "\n",
        "    out[\"ema20\"] = ta.trend.EMAIndicator(out[\"close\"].squeeze(), window=20).ema_indicator().values.reshape(-1, 1)\n",
        "    out[\"ma5\"] = out[\"close\"].rolling(5).mean()\n",
        "    out[\"ma10\"] = out[\"close\"].rolling(10).mean()\n",
        "\n",
        "    out[\"mtm6\"] = out[\"close\"] - out[\"close\"].shift(6)\n",
        "    out[\"mtm12\"] = out[\"close\"] - out[\"close\"].shift(12)\n",
        "\n",
        "    out[\"roc\"] = ta.momentum.ROCIndicator(out[\"close\"].squeeze(), window=10).roc().values.reshape(-1, 1)\n",
        "\n",
        "    # SMI: 簡易実装（EMA二重平滑）\n",
        "    hl = (out[\"high\"] + out[\"low\"]) / 2\n",
        "    diff = out[\"close\"] - hl\n",
        "    hl_range = out[\"high\"] - out[\"low\"]\n",
        "    ema1 = diff.ewm(span=14, adjust=False).mean()\n",
        "    ema2 = ema1.ewm(span=14, adjust=False).mean()\n",
        "    range_ema1 = hl_range.ewm(span=14, adjust=False).mean()\n",
        "    range_ema2 = range_ema1.ewm(span=14, adjust=False).mean()\n",
        "    out[\"smi\"] = 100 * (ema2 / (0.5 * range_ema2.replace(0, np.nan)))\n",
        "\n",
        "    # WVAD: Williams Variable Accumulation/Distribution\n",
        "    denom = (out[\"high\"] - out[\"low\"]).replace(0, np.nan)\n",
        "    out[\"wvad\"] = ((out[\"close\"] - out[\"open\"]) / denom) * out[\"volume\"]\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def remove_high_corr_features(df: pd.DataFrame, target_col: str, threshold: float = 0.95) -> tuple[pd.DataFrame, list]:\n",
        "    corr_series = df.corr(numeric_only=True)[target_col].abs()\n",
        "    if isinstance(corr_series, pd.DataFrame):\n",
        "        corr_series = corr_series.squeeze()\n",
        "    vals = np.asarray(corr_series).ravel()\n",
        "    names = list(corr_series.index)\n",
        "    is_high = (vals > threshold).astype(bool)\n",
        "    not_self = np.array([n != target_col for n in names], dtype=bool)\n",
        "    mask = is_high & not_self\n",
        "    drop_cols = [names[i] for i in range(len(names)) if mask[i]]\n",
        "    return df.drop(columns=drop_cols), drop_cols\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b07078",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_target_returns(df: pd.DataFrame) -> pd.Series:\n",
        "    close = pd.Series(np.asarray(df[\"close\"]).ravel(), index=df.index)\n",
        "    returns = close.pct_change().shift(-1)\n",
        "    returns.name = \"target_return\"\n",
        "    return returns\n",
        "\n",
        "\n",
        "def create_sequences(features: np.ndarray, target: np.ndarray, lookback: int):\n",
        "    xs, ys = [], []\n",
        "    for i in range(lookback, len(features)):\n",
        "        xs.append(features[i - lookback : i])\n",
        "        ys.append(target[i])\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "\n",
        "def train_val_test_split(X, y, train_ratio=0.8, val_ratio=0.2):\n",
        "    n = len(X)\n",
        "    train_end = int(n * train_ratio)\n",
        "    val_end = int(train_end * (1 - val_ratio))\n",
        "\n",
        "    X_train = X[:val_end]\n",
        "    y_train = y[:val_end]\n",
        "    X_val = X[val_end:train_end]\n",
        "    y_val = y[val_end:train_end]\n",
        "    X_test = X[train_end:]\n",
        "    y_test = y[train_end:]\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "\n",
        "def scale_train_val_test(X_train, X_val, X_test, y_train, y_val, y_test):\n",
        "    n_features = X_train.shape[2]\n",
        "\n",
        "    x_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    X_train_s = x_scaler.fit_transform(X_train.reshape(-1, n_features)).reshape(X_train.shape)\n",
        "    X_val_s = x_scaler.transform(X_val.reshape(-1, n_features)).reshape(X_val.shape)\n",
        "    X_test_s = x_scaler.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape)\n",
        "\n",
        "    y_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    y_train_s = y_scaler.fit_transform(y_train.reshape(-1, 1))\n",
        "    y_val_s = y_scaler.transform(y_val.reshape(-1, 1))\n",
        "    y_test_s = y_scaler.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    return X_train_s, X_val_s, X_test_s, y_train_s, y_val_s, y_test_s, x_scaler, y_scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bebcdd46",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_lstm_model(input_shape, num_layers: int, num_units: int):\n",
        "    model = keras.Sequential()\n",
        "    for i in range(num_layers):\n",
        "        return_sequences = i < num_layers - 1\n",
        "        if i == 0:\n",
        "            model.add(layers.LSTM(num_units, return_sequences=return_sequences, input_shape=input_shape))\n",
        "        else:\n",
        "            model.add(layers.LSTM(num_units, return_sequences=return_sequences))\n",
        "        model.add(layers.Dropout(0.2))\n",
        "\n",
        "    model.add(layers.Dense(1))\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdc1af77",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pso_optimize(X_train, y_train, X_val, y_val, input_shape):\n",
        "    def _objective(particles):\n",
        "        costs = []\n",
        "        for particle in particles:\n",
        "            units = int(np.clip(round(particle[0]), *NEURON_BOUNDS))\n",
        "            epochs = int(np.clip(round(particle[1]), *EPOCH_BOUNDS))\n",
        "            layers = int(np.clip(round(particle[2]), *LAYER_BOUNDS))\n",
        "\n",
        "            tf.keras.backend.clear_session()\n",
        "            model = build_lstm_model(input_shape, layers, units)\n",
        "            es = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "            model.fit(\n",
        "                X_train,\n",
        "                y_train,\n",
        "                validation_data=(X_val, y_val),\n",
        "                epochs=epochs,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                verbose=0,\n",
        "                callbacks=[es],\n",
        "            )\n",
        "            preds = model.predict(X_val, verbose=0)\n",
        "            rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
        "            costs.append(rmse)\n",
        "        return np.array(costs)\n",
        "\n",
        "    options = {\"c1\": PSO_C1, \"c2\": PSO_C2, \"w\": PSO_W}\n",
        "    lower_bounds = np.array([NEURON_BOUNDS[0], EPOCH_BOUNDS[0], LAYER_BOUNDS[0]])\n",
        "    upper_bounds = np.array([NEURON_BOUNDS[1], EPOCH_BOUNDS[1], LAYER_BOUNDS[1]])\n",
        "    bounds = (lower_bounds, upper_bounds)\n",
        "\n",
        "    optimizer = ps.single.GlobalBestPSO(\n",
        "        n_particles=PSO_PARTICLES,\n",
        "        dimensions=3,\n",
        "        options=options,\n",
        "        bounds=bounds,\n",
        "    )\n",
        "    best_cost, best_pos = optimizer.optimize(_objective, iters=PSO_ITERS, verbose=False)\n",
        "    return best_pos, best_cost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92171d9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def resample_series(series: pd.Series, minutes: int) -> pd.Series:\n",
        "    if minutes == 5:\n",
        "        return series.copy()\n",
        "    rule = f\"{minutes}min\"\n",
        "    return series.resample(rule).last().dropna()\n",
        "\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    eps = 1e-8\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + eps))) * 100\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return rmse, mae, mape, r2\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "for ticker in INDEX_TICKERS:\n",
        "    print(f\"\\n=== {ticker} ===\")\n",
        "    base_price = download_price_data(ticker, BASE_INTERVAL, BASE_PERIOD)\n",
        "\n",
        "    try:\n",
        "        fx_5m = download_price_data(FX_TICKER, BASE_INTERVAL, BASE_PERIOD)[\"close\"]\n",
        "    except Exception:\n",
        "        fx_5m = download_macro_daily(FX_TICKER)\n",
        "\n",
        "    rate_daily = download_macro_daily(RATE_TICKER)\n",
        "\n",
        "    for minutes in RESAMPLE_MINUTES:\n",
        "        print(f\"--- {minutes}分足 ---\")\n",
        "        df = resample_ohlcv(base_price, minutes)\n",
        "        df[\"close\"] = wavelet_denoise(df[\"close\"], level=3)\n",
        "\n",
        "        df = add_technical_indicators(df)\n",
        "\n",
        "        fx_resampled = resample_series(fx_5m, minutes)\n",
        "        df = merge_macro_features(df, fx_resampled, rate_daily)\n",
        "\n",
        "        df[\"target_return\"] = build_target_returns(df)\n",
        "        df = df.dropna()\n",
        "\n",
        "        feature_df, dropped = remove_high_corr_features(df.drop(columns=[\"target_return\"]), target_col=\"close\")\n",
        "        df_final = feature_df.copy()\n",
        "        df_final[\"target_return\"] = df[\"target_return\"]\n",
        "\n",
        "        feature_cols = [c for c in df_final.columns if c != \"target_return\"]\n",
        "        features = df_final[feature_cols].values\n",
        "        target = df_final[\"target_return\"].values\n",
        "\n",
        "        lookback = LOOKBACK_STEPS[minutes]\n",
        "        X, y = create_sequences(features, target, lookback)\n",
        "\n",
        "        X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(\n",
        "            X, y, train_ratio=TRAIN_RATIO, val_ratio=VAL_RATIO\n",
        "        )\n",
        "\n",
        "        X_train_s, X_val_s, X_test_s, y_train_s, y_val_s, y_test_s, x_scaler, y_scaler = scale_train_val_test(\n",
        "            X_train, X_val, X_test, y_train, y_val, y_test\n",
        "        )\n",
        "\n",
        "        input_shape = (X_train_s.shape[1], X_train_s.shape[2])\n",
        "\n",
        "        best_pos, best_cost = pso_optimize(X_train_s, y_train_s, X_val_s, y_val_s, input_shape)\n",
        "        best_units = int(np.clip(round(best_pos[0]), *NEURON_BOUNDS))\n",
        "        best_epochs = int(np.clip(round(best_pos[1]), *EPOCH_BOUNDS))\n",
        "        best_layers = int(np.clip(round(best_pos[2]), *LAYER_BOUNDS))\n",
        "\n",
        "        tf.keras.backend.clear_session()\n",
        "        final_model = build_lstm_model(input_shape, best_layers, best_units)\n",
        "        final_model.fit(\n",
        "            np.concatenate([X_train_s, X_val_s]),\n",
        "            np.concatenate([y_train_s, y_val_s]),\n",
        "            epochs=best_epochs,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            verbose=0,\n",
        "        )\n",
        "\n",
        "        pred_scaled = final_model.predict(X_test_s, verbose=0)\n",
        "        y_pred = y_scaler.inverse_transform(pred_scaled).reshape(-1)\n",
        "        y_true = y_scaler.inverse_transform(y_test_s).reshape(-1)\n",
        "\n",
        "        rmse, mae, mape, r2 = compute_metrics(y_true, y_pred)\n",
        "\n",
        "        results.append(\n",
        "            {\n",
        "                \"ticker\": ticker,\n",
        "                \"minutes\": minutes,\n",
        "                \"lookback\": lookback,\n",
        "                \"units\": best_units,\n",
        "                \"epochs\": best_epochs,\n",
        "                \"layers\": best_layers,\n",
        "                \"rmse\": rmse,\n",
        "                \"mae\": mae,\n",
        "                \"mape\": mape,\n",
        "                \"r2\": r2,\n",
        "                \"dropped_features\": dropped,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"units={best_units}, epochs={best_epochs}, layers={best_layers} | \"\n",
        "            f\"RMSE={rmse:.6f}, MAE={mae:.6f}, MAPE={mape:.4f}, R2={r2:.4f}\"\n",
        "        )\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73212f7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df = results_df.sort_values([\"ticker\", \"minutes\"]).reset_index(drop=True)\n",
        "results_df\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
